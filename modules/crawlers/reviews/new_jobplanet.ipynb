{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawling\n",
    "import requests as req\n",
    "from user_agent import generate_navigator # 랜덤 헤더 생성 모듈\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# to sign in\n",
    "from accounts import jp\n",
    "import json\n",
    "\n",
    "\n",
    "# to dataframe\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# async\n",
    "import asyncio\n",
    "import aiohttp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 선언부\n",
    "JOBPLANET_URL = 'https://www.jobplanet.co.kr'\n",
    "JOBPLANET_LOGIN_URL = JOBPLANET_URL + '/users/sign_in'\n",
    "JOBPLANET_SEARCH_URL = JOBPLANET_URL + '/search?query={keyword}'\n",
    "JOBPLANET_REVIEW_URL = JOBPLANET_URL + '/companies/{jp_comp_uid}/reviews?page={p}'\n",
    "#SAVE_PATH = '/app/data/reviews/{comp_name}_job_planet.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make request datas\n",
    "def get_login_session():\n",
    "    \"\"\"\n",
    "    잡플래닛 로그인을 시도해, request session 객체와 cookie 값을 반환합니다.\n",
    "    \n",
    "    returns ]\n",
    "        session : request.sessions.Session  - 로그인 된 세션\n",
    "        cookies : dict                      - 로그인 완료된 쿠키값\n",
    "    \"\"\"\n",
    "    # ==============================\n",
    "    # Header\n",
    "    # ==============================\n",
    "    # 랜덤으로 돌립니다.\n",
    "    while True:\n",
    "        HEADERS = generate_navigator(device_type=\"desktop\", os=('mac', 'linux', 'win'))\n",
    "        if None not in HEADERS.values():\n",
    "            break\n",
    "        \n",
    "    # 나머지 헤더값 넣어주기\n",
    "    HEADERS['Referer'] = 'https://www.jobplanet.co.kr/users/sign_in?_nav=gb'\n",
    "    HEADERS['Origin'] = 'https://www.jobplanet.co.kr'\n",
    "    HEADERS['Content-Type'] = 'application/json'\n",
    "    HEADERS['Accpet'] = '*/*'\n",
    "    HEADERS['Accpet-Encoding'] = 'gzip, deflate, br'\n",
    "    HEADERS['Accept-Language'] = 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'\n",
    "    \n",
    "    # ==============================\n",
    "    # Payload\n",
    "    # ==============================\n",
    "    PAYLOAD = {\n",
    "        'user': {\n",
    "            'email': jp.ID,\n",
    "            'password': jp.PW,\n",
    "            'remember_me': False  \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # ==============================\n",
    "    # make session to use cookie\n",
    "    # ==============================\n",
    "    session = req.Session()\n",
    "    login_cookie = session.post(JOBPLANET_LOGIN_URL, data=json.dumps(PAYLOAD), headers=HEADERS).cookies.get_dict()\n",
    "    # update cookies & headers\n",
    "    session.cookies.update(login_cookie)\n",
    "    session.headers.update(HEADERS)\n",
    "    \n",
    "    return session, session.cookies.get_dict() # 쿠키 따로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__cf_bm': 'ORkqvvBwB5w91LOi_m2e88PX90Qzqpdoke3QV_uWxyE-1692851549-0-AX/kkze385t0YoQOUeqDnyVWg2iJBn95hjLr10YV4j8MpmxAM6kfOnU3BMdLL2CuRQ9OsP9h/tuJPMfVO8tfr3g=',\n",
       " '_intween_x2o_net_session': 'SW5hdGhjeEtoMDRKVExUKzJUdnJZS1orSlc0aWtaWUhkK05EODZvUUhFanU0b1prOFZOL3huZTdCYnZvZVAwY2tvSjVJRVVMa0xnU2pMd2xweWdWNlkvNHkyYlhsMFExNU1wZ3pJdmJUS044dkxzeGRoRmtacXUvMzBpZndpVTdnRUptWWp5ZjNzVVMxdmc5cmw2TXByT1Y2S240V0Z5cXJ5WHVUZkxLdjFFPS0tQ3VDMHptdmRnTDdHdkVQNE02eURldz09--40663360bc1aab642fb8a2f54fa542d4569f6fc1',\n",
       " '_jp_traffic_source': '%7B%22utm_campaign%22%3Anull%2C%22utm_medium%22%3Anull%2C%22utm_source%22%3Anull%7D',\n",
       " '_jp_visit_short_token': '1692851548879-7f78660e-e4b7-4a4b-a325-7898693d30f4',\n",
       " '_jp_visit_token': 'bdf8ea17-92b9-48f0-a052-fb897e489f99',\n",
       " '_jp_visitor_token': 'e28de7a4-5aef-40af-bfd6-bda9d5a5da09',\n",
       " 'request_method': 'POST'}"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확인용\n",
    "session.cookies.get_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jobplanet_uid(headers, keyword):\n",
    "    # 크롤링 준비\n",
    "    res = req.get(JOBPLANET_SEARCH_URL.format(keyword=keyword), headers=headers)\n",
    "    soup = bs(res.text, 'lxml')\n",
    "\n",
    "    # 잡플래닛 내부 회사 ID 크롤링\n",
    "    # b 태그 갖고 있는 a 태그만 추출 : 정확도 높은게 볼드체 처리됨\n",
    "    a_tag = [el for el in soup.select('div.is_company_card a') if el.select_one('b')][0]\n",
    "\n",
    "    # href format : /companies/{잡플래닛_회사ID}/info/{회사이름}?_rs_act=index&_rs_con=search&_rs_element=federated_search\n",
    "    return a_tag.attrs['href'].split('/')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_to_keyword(headers, jp_comp_uid):\n",
    "    \"\"\"\n",
    "    제목까지 받아와야 하는 경우, 해당 함수의 주석과 get_news_list 함수의 주석을 해제 후 사용해야 합니다.\n",
    "    parameter ]\n",
    "        headers : dict  - header 정보\n",
    "        keyword : str   - 해당 단어로 다음 검색(입력시 global 변수로 저장)\n",
    "        \n",
    "    return ]\n",
    "        links   : list[str] - 리뷰 페이지들 링크\n",
    "    \"\"\"\n",
    "    review_list = []\n",
    "    p = 1\n",
    "    \n",
    "    for p in range(99):\n",
    "        now_url = JOBPLANET_REVIEW_URL.format(jp_comp_uid=jp_comp_uid, p=p+1)\n",
    "    \n",
    "        res = req.get(now_url, headers=headers)\n",
    "        soup = bs(res.text, 'lxml')\n",
    "        \n",
    "        # 리뷰 없는 경우\n",
    "        if soup.select_one('article.no_result > .txt'): # '리뷰가 없습니다' 태그\n",
    "            break\n",
    "        \n",
    "        # 통과한 경우 : 리뷰 있음, 리스트에 추가\n",
    "        review_list.append(now_url)\n",
    "    \n",
    "    return review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_df(session, url):\n",
    "    # 크롤링 준비\n",
    "    # DataFrame 생성 list들\n",
    "    position_list = []\n",
    "    is_office_list = []\n",
    "    review_date_list = []\n",
    "    review_rate_list = []\n",
    "    review_cont_list = []\n",
    "    \n",
    "    res = session.get(JOBPLANET_REVIEW_URL.format(jp_comp_uid=jp_comp_uid, v_page=page), headers=HEADERS)\n",
    "    soup = bs(res.text, 'lxml')\n",
    "    \n",
    "    # =========================\n",
    "    # 작성자 정보 크롤링\n",
    "    # =========================\n",
    "    reviewer_info_list = []\n",
    "    # span.txt1 : [직무, (전직원|현직원), 지역, 작성 날짜] 의 연속으로 구성됨\n",
    "    [reviewer_info_list.append(el.text.strip()) for el in soup.select('span.txt1')]\n",
    "\n",
    "        \n",
    "        \n",
    "    # filtering from reviewer_info_list\n",
    "    # 장점 / 단점 각각 정보가 들어가야 하기 때문에 두 번씩 들어가게 한다.\n",
    "    for _ in range(2):\n",
    "        position_list.extend(reviewer_info_list[::4])\n",
    "        is_office_list.extend([False if el == '전직원' else True for el in reviewer_info_list[1::4]])\n",
    "        # YYYY. MM -> YYYYMM\n",
    "        review_date_list.extend([el.replace('. ', '') for el in reviewer_info_list[3::4]])\n",
    "        \n",
    "    # =========================\n",
    "    # 리뷰 크롤링\n",
    "    # =========================\n",
    "    # 별점\n",
    "    # width:{percent}% -> {percent} -> 1~5점 사이로 format\n",
    "    # 장점 / 단점 각각 정보가 들어가야 하기 때문에 두 번씩 들어가게 한다.\n",
    "    for _ in range(2):\n",
    "        review_rate_list.extend([int(int(el.attrs['style'][6:-2]) / 20) for el in soup.select('div.star_score')])\n",
    "    \n",
    "    # 리뷰 내용 : react로 불러오기 때문에 데이터만 불러와 줍니다.\n",
    "    review_cont_list.extend([el for el in soup.select('dt.merit+dd > span')]) # 장점\n",
    "    review_cont_list.extend([el for el in soup.select('dt.disadvantages+dd > span')]) # 단점\n",
    "    \n",
    "    \n",
    "    return position_list, is_office_list,  review_date_list, review_rate_list, review_cont_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_reviewer_info(one_reviewer_info):\n",
    "    \"\"\"\n",
    "    한 리뷰어의 정보를 list로 받아, split해주는 함수\n",
    "    parameters ]\n",
    "        one_reviewer_info : list    - 리뷰어 한명의 정보\n",
    "        \n",
    "    return ]\n",
    "        position    : str   - 직무\n",
    "        is_office   : str   - 현직원/전직원 여부\n",
    "        review_date : str   - 리뷰 작성 날짜\n",
    "    \"\"\"\n",
    "    position = one_reviewer_info[0]\n",
    "    is_office = False if one_reviewer_info[1] == '전직원' else True\n",
    "    review_date = one_reviewer_info[3].replace('. ', '')\n",
    "    \n",
    "    return position, is_office, review_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['기획/경영', '전직원', '서울', '2023. 07'],\n",
       " ['미디어/홍보', '현직원', '서울', '2023. 06'],\n",
       " ['금융/재무', '현직원', '서울', '2023. 02'],\n",
       " ['개발', '현직원', '서울', '2023. 01'],\n",
       " ['개발', '현직원', '서울', '2023. 01']]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 함수만 테스트\n",
    "url = r'https://www.jobplanet.co.kr/companies/366153/reviews/%EB%A0%88%EB%AA%AC?page=14'\n",
    "\n",
    "# 크롤링 준비\n",
    "res = req_session.get(urls[0], headers=req_session.headers)\n",
    "soup = bs(res.text, 'lxml')\n",
    "\n",
    "# =========================\n",
    "# 작성자 정보 크롤링\n",
    "# =========================\n",
    "reviewer_info_list = []\n",
    "# span.txt1 : [직무, (전직원|현직원), 지역, 작성 날짜] 의 연속으로 구성됨\n",
    "[reviewer_info_list.append(el.text.strip()) for el in soup.select('span.txt1')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_cont_list ::::: 5\n",
      "review_cont_list ::::: 5\n",
      "review_rate_list ::::: 136\n",
      "is_office_list ::::: 136\n",
      "review_date_list ::::: 136\n",
      "position_list ::::: 136\n"
     ]
    }
   ],
   "source": [
    "print('review_cont_list :::::', len(review_pos_list))\n",
    "print('review_cont_list :::::', len(review_neg_list))\n",
    "print('review_rate_list :::::', len(review_rate_list))\n",
    "print('is_office_list :::::', len(is_office_list))\n",
    "print('review_date_list :::::', len(review_date_list))\n",
    "print('position_list :::::', len(position_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_content_list(cookies, urls):\n",
    "    async with aiohttp.ClientSession(\n",
    "        connector=aiohttp.TCPConnector(ssl=False)\n",
    "    ) as session:\n",
    "        session.cookie_jar.update_cookies(cookies, response_url=None)\n",
    "        result = await asyncio.gather(*[get_content_to_link(session, url) for url in urls]) # wrapping도 내부 처리\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\AppData\\Local\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:314: RuntimeWarning: coroutine 'get_content_list' was never awaited\n",
      "  for attr in list(attrs.keys()):\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# 테스트\n",
    "keyword = '지쿱'\n",
    "req_session, req_cookies = get_login_session()\n",
    "jp_comp_uid = get_jobplanet_uid(req_session.headers, keyword)\n",
    "urls = get_links_to_keyword(req_session.headers, jp_comp_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[260], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# 데이터 크롤링\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m result \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39;49mrun(get_content_list(req_session\u001b[39m.\u001b[39;49mcookies, urls))\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\lib\\asyncio\\runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[39mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39mif\u001b[39;00m events\u001b[39m.\u001b[39m_get_running_loop() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m     34\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m coroutines\u001b[39m.\u001b[39miscoroutine(main):\n\u001b[0;32m     37\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39ma coroutine was expected, got \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(main))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "# 데이터 크롤링\n",
    "result = asyncio.run(get_content_list(req_cookies, urls))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ezpz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

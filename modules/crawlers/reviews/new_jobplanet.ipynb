{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from user_agent import generate_navigator\n",
    "\n",
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "# jp.ID, jp.PW\n",
    "from accounts import jp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\n",
    "HEADERS = {'User-Agent' : user_agent}\n",
    "\n",
    "############################\n",
    "# 보낼 데이터\n",
    "############################\n",
    "payload = {\n",
    "    'user': {\n",
    "        'email': jp.ID,\n",
    "        'password': jp.PW,\n",
    "        'remember_me': False  \n",
    "    }\n",
    "}\n",
    "\n",
    "############################\n",
    "# payload to json\n",
    "############################\n",
    "payload = json.dumps(payload)\n",
    "payload\n",
    "\n",
    "############################\n",
    "# 헤더 생성\n",
    "############################\n",
    "while True:\n",
    "    headers_ori = generate_navigator(device_type=\"desktop\", os=('mac', 'linux', 'win'))\n",
    "    if None not in headers_ori.values():\n",
    "        break\n",
    "\n",
    "############################\n",
    "# 로그인용 헤더로 복사\n",
    "############################\n",
    "headers_login = dict(headers_ori)\n",
    "\n",
    "############################\n",
    "# 로그인 헤더에 값 추가\n",
    "############################\n",
    "headers_login['Referer'] = 'https://www.jobplanet.co.kr/users/sign_in?_nav=gb'\n",
    "headers_login['Origin'] = 'https://www.jobplanet.co.kr'\n",
    "headers_login['Content-Type'] = 'application/json'\n",
    "headers_login['Accept'] = '*/*'\n",
    "headers_login['Accept-Encoding'] = 'gzip, deflate, br'\n",
    "headers_login['Accept-Language'] = 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'\n",
    "# headers_login['Content-Length'] = '100'\n",
    "\n",
    "login_url = 'https://www.jobplanet.co.kr/users/sign_in'\n",
    "\n",
    "############################\n",
    "# 세션 생성\n",
    "############################\n",
    "session = requests.Session()\n",
    "\n",
    "############################\n",
    "# 로그인 쿠키와 로그인 헤더를 이용해 로그싱 포스트 요청\n",
    "############################\n",
    "res = session.post(login_url, data=payload, headers=headers_login)\n",
    "html = res.text\n",
    "print(html)\n",
    "# html 값 확인 : 로그인 성공시 json 형태로 리턴됨\n",
    "# {\"success\":true,\"redirect_url\":\"/\",\"di_resu\":4445780}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 회사 검색 후 첫번째 회사로 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# 회사 검색창 들어간 후 첫번째꺼 가져오는 함수\n",
    "############################\n",
    "def get_comp_review(url):\n",
    "    #requests로 url 접근 요청\n",
    "    r = requests.get(url, headers=HEADERS)\n",
    "\n",
    "    #r변수에 저장된 content를 파싱해서 soup객체에  content 저장\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "    #req에 저장한 html을 파싱해서 soup 이라는 변수에 저장\n",
    "    #is_company_card 클래스인 div 태그에서 a 붙은 태그(a href 찾음)\n",
    "    tag_text = soup.find('div', class_=\"is_company_card\").find_all('a')\n",
    "\n",
    "    # 찾은 a태그 애들중에 첫번쨰 a태그(첫번째 기업) str로 변환 후에 tag\n",
    "    tag_text=str(tag_text[0])\n",
    "\n",
    "    # '/'로 분할 후 3번째 문자열(찾은 태그에서 뽑고자 하는 기업 번호)\n",
    "    comp_uid = tag_text.split('/')[2]\n",
    "\n",
    "    # 찾고자 하는 기업번호가 추가된 최종 url\n",
    "    new_url= f'https://www.jobplanet.co.kr/companies/{comp_uid}/reviews'\n",
    "\n",
    "    #기업 번호 반환과 tag_return 튜플로 반환 튜플 인덱싱으로 uid 크롤링때는 tag_return, new_url은 접속용\n",
    "    return get_comp_review_crawl(new_url,comp_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_to_get_review = {\n",
    "    'www.jobplanet.co.kr': get_comp_review #jopplanet url 인지 확인후 get_comp_review함수 실행시켜줄 딕셔너리\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "#회사 정보 요청 함수\n",
    "############################\n",
    "def get_info(url):\n",
    "    #도메인주소만 가져오는 코드 www.jobplanet.co.kr\n",
    "    site_url = url.split('/')[2]\n",
    "\n",
    "    # 전역변수 딕셔너리에서 있는 키값이랑 site_url이 동일하면 value 아니면 None\n",
    "    get_content_func = link_to_get_review.get(site_url, None)\n",
    "    if get_content_func:\n",
    "        return get_comp_review(url)\n",
    "    else:\n",
    "        return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# 회사 이름 입력받아 url로 만들어주는 함수\n",
    "############################\n",
    "def get_url(comp):\n",
    "    url = f'https://www.jobplanet.co.kr/search?query={comp}' #여기에 회사 이름 추가해야됨\n",
    "    return get_info(url) \n",
    "\n",
    "# 로그인 성공한 res객체에서 쿠키 받아오기\n",
    "cookie = res.cookies.get_dict()\n",
    "\n",
    "# 새로운 세션 생성\n",
    "session = requests.Session()\n",
    "# 쿠키 추가\n",
    "session.cookies.update(cookie)\n",
    "# 원본 헤더 쿠키\n",
    "session.headers.update(headers_ori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 크롤러"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# 리뷰 크롤링 해오는 함수\n",
    "############################\n",
    "\n",
    "def get_comp_review_crawl(new_url, comp_uid):\n",
    "    print(\"review_crawl 실행됨\")\n",
    "    review_list = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        # 현재 페이지의 URL 생성 및 요청\n",
    "        url = f'https://www.jobplanet.co.kr/companies/{comp_uid}/reviews/?page={page}'\n",
    "        print(url)\n",
    "        # #requests로 url 접근 요청\n",
    "        r = requests.get(url, headers=HEADERS)\n",
    "\n",
    "        # r변수에 저장된 content를 파싱해서 soup객체에  content 저장\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        #추출해야할 정보들 죄다 zip로 묶어줌 uid는 저위에 comp_uid\n",
    "\n",
    "        # 회사 uid\n",
    "        # uid는 get_comp_review 함수에서 인자로 받음\n",
    "\n",
    "        no_review_message = soup.select_one(\"#viewReviewsList > div > div > div > article > p\")\n",
    "        if no_review_message and \"등록된 기업리뷰가 없습니다.\" in no_review_message.get_text():\n",
    "            print(\"기업리뷰없음.\")\n",
    "            break\n",
    "\n",
    "        # 리뷰들을 돌면서 데이터 추출\n",
    "        reviews = soup.select(\"section > div > div.ctbody_col2 > div\")\n",
    "\n",
    "        for review in reviews:\n",
    "            review_data = {}\n",
    "\n",
    "\n",
    "            # 리뷰 작성자 정보\n",
    "            # 직무\n",
    "            elements_reviewer_info = soup.find_all('span', class_=\"txt1\")\n",
    "            reviewer_texts = [element_reviewer_info.get_text() for element_reviewer_info in elements_reviewer_info]\n",
    "\n",
    "            positions = []\n",
    "            is_offices = []\n",
    "            review_dates = []\n",
    "\n",
    "            for i in range(0, len(reviewer_texts), 4):\n",
    "                position_elements = reviewer_texts[i] if len(reviewer_texts) > i else None\n",
    "                position = re.sub(r\"\\s\", \"\", position_elements) if position_elements else None\n",
    "                positions.append(positions)\n",
    "\n",
    "                is_office_elements = reviewer_texts[i+1] if len(reviewer_texts) > i+1 else None\n",
    "                is_office = re.sub(r\"\\s\", \"\", is_office_elements) if is_office_elements else None\n",
    "                is_offices.append(is_office)\n",
    "\n",
    "                review_date_elements = reviewer_texts[i+3] if len(reviewer_texts) > i+3 else None\n",
    "                review_date = re.sub(r\"[^0-9]\", \"\", review_date_elements) if review_date_elements else None\n",
    "                review_dates.append(review_date)\n",
    "\n",
    "            print(positions)\n",
    "            print(is_offices)\n",
    "            print(review_dates)\n",
    "\n",
    "            # 별점\n",
    "            elements_review_rate = soup.find_all('div', class_=\"star_score\")\n",
    "\n",
    "            review_rates = []\n",
    "            \n",
    "            for element in elements_review_rate:\n",
    "                style_value = element.get('style')\n",
    "                if style_value is not None:\n",
    "                    match = re.search(r\"\\d+\", style_value)\n",
    "                    if match is not None:\n",
    "                        rate = float(match.group()) / 20\n",
    "                        review_rates.append(rate)\n",
    "\n",
    "            print(review_rates)\n",
    "\n",
    "            # 장점 단점\n",
    "            # elements_reviewer_info = soup.find_all('span', class_=\"txt1\")\n",
    "\n",
    "            # review_conts = []\n",
    "            # review_sentis = []\n",
    "            # texts = [element_reviewer_info.get_text() for element_reviewer_info in elements_reviewer_info]\n",
    "            # print(texts)\n",
    "            # good_review_cont_elements = soup.select('section > div > div.ctbody_col2 > div > dl > dd:nth-child(2) > span')\n",
    "            # if good_review_cont_elements:\n",
    "            #     good_review_cont = clean_str(good_review_cont_elements[0].get_text().strip())\n",
    "            \n",
    "            # bad_review_cont_elements = soup.select('section > div > div.ctbody_col2 > div > dl > dd:nth-child(4) > span')\n",
    "            # if bad_review_cont_elements:\n",
    "            #     bad_review_cont = clean_str(good_review_cont_elements[0].get_text().strip())\n",
    "\n",
    "            review_list.append(review_data)\n",
    "\n",
    "            comp_review_dict={\n",
    "                'comp_uid' : comp_uid,\n",
    "                # 'review_cont': review_cont,\n",
    "                # 'review_senti' : review_senti,\n",
    "                'good_review_cont': good_review_cont,\n",
    "                'bad_review_cont': bad_review_cont,\n",
    "                'review_rate' : review_rate,\n",
    "                'is_office' : is_office,\n",
    "                'review_date': review_date,\n",
    "                'position' : position,\n",
    "                }\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    #기업 정보 key, value로 담은 딕셔너리\n",
    "    return get_comp_review_df(comp_review_dict) #기업정보들 튜플로 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comp_review_df(comp_review_dict):\n",
    "    print(\"df 실행됨\")\n",
    "    # save_path = f\"/Users/gyeonmunju/Desktop/PlayData/프로젝트_정리파일/request_change\"\n",
    "    # os.makedirs(save_path, exist_ok=True)\n",
    "    df = pd.DataFrame.from_dict({'':comp_review_dict}, orient='index') #데이터를 데이터프레임으로 저장\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_url(\"(주)레몬\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

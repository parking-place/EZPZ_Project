{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: beautifulsoup4 in c:\\users\\playdata\\anaconda3\\envs\\ezpz\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\playdata\\anaconda3\\envs\\ezpz\\lib\\site-packages (from beautifulsoup4) (2.4.1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'\n",
    "HEADERS = {'User-Agent' : user_agent}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "## 회사 기업정보 페이지 접속\n",
    "\n",
    "### 접속인데 가져오는 함수 이름으로 돼있으므로 함수이름 정정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "#케이리포트 크롤링 url 검색어 포함 아니라 어렵고 로고 없어서 잡플래닛으로 변경\n",
    "\n",
    "# 회사 정보 가져오는 함수 (잡플래닛)\n",
    "def get_comp_info(url):\n",
    "    r = requests.get(url, headers=HEADERS) #requests로 url 접근 요청\n",
    "    soup = BeautifulSoup(r.content, 'html.parser') #r변수에 저장된 content를 파싱해서 soup객체에  content 저장\n",
    "    #req에 저장한 html을 파싱해서 soup 이라는 변수에 저장\n",
    "    tag_text=soup.find('div', class_=\"is_company_card\").find_all('a') #is_company_card 클래스인 div 태그에서 a 붙은 태그(a href 찾음)\n",
    "    tag_text=str(tag_text[0]) # 찾은 a태그 애들중에 첫번쨰 a태그(첫번째 기업) str로 변환 후에 tag\n",
    "    comp_uid = tag_text.split('/')[2] # '/'로 분할 후 3번째 문자열(찾은 태그에서 뽑고자 하는 기업 번호)\n",
    "    new_url= f'https://www.jobplanet.co.kr/companies/{comp_uid}/landing'# 찾고자 하는 기업번호가 추가된 최종 url\n",
    "    return get_comp_info_crawl(new_url,comp_uid)  #기업 번호 반환과 tag_return 튜플로 반환 튜플 인덱싱으로 uid 크롤링때는 tag_return, new_url은 접속용\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_to_get_info = {\n",
    "    'www.jobplanet.co.kr': get_comp_info #jopplanet url 인지 확인후 get_comp_info함수 실행시켜줄 딕셔너리\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "#회사 정보 요청 함수\n",
    "def get_info(url):\n",
    "    \n",
    "    site_url = url.split('/')[2] #도메인주소만 가져오는 코드 www.jobplanet.co.kr\n",
    "    get_content_func = link_to_get_info.get(site_url, None) # 전역변수 딕셔너리에서 있는 키값이랑 site_url이 동일하면 value 아니면 None \n",
    "    if get_content_func:\n",
    "        return get_comp_info(url)\n",
    "    else:\n",
    "        return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 회사 이름 입력받아 url로 만들어주는 함수 \n",
    "def get_url(comp):\n",
    "    url=f'https://www.jobplanet.co.kr/search?query={comp}' #여기에 회사 이름 추가해야됨\n",
    "    return get_info(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## 기업 정보 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "#찾으려고하는 컨텐츠가 없을 시 -로 처리됨 확실한진 하나하나 체크\n",
    "#로고는 없는 로고일시 공통 빈로고 이미지를 띄움\n",
    "\n",
    "'''\n",
    "회사 uid (회사 고유 기업번호로) /get_url로 구하기 완료\n",
    "회사 이름 (맨 상단 주식회사 이름) 완료\n",
    "회사 주소 (주소) 완료\n",
    "회사 로고 (이미지 주소로 저장) 완료\n",
    "사업 내용 (산업) 완료\n",
    "회사 설립 년월(설립(df로 저장하고 나중에 저장형식대로 .빼기),저장은 YYYYMM형식의 string) 완료\n",
    "회사 규모((기업형태)중소/대 기업등) 완료\n",
    "기업 대표 사이트(웹사이트 링크 주기 링크 없으면 예외처리) \n",
    "처리 여부(처리되면 Y 아니면 N)\n",
    "최초 저장 일자 (YYYYMMDD)\n",
    "수정 일자 (YYYYMMDD) \n",
    "'''\n",
    "def get_comp_info_crawl(new_url,comp_uid): #get_url(기업명)실행으로 받은 기업정보 url로 접속 테스트는 그냥 바로 기업정보 url로 해보기\n",
    "    r = requests.get(new_url, headers=HEADERS) #requests로 url 접근 요청\n",
    "    soup = BeautifulSoup(r.content, 'html.parser') #r변수에 저장된 content를 파싱해서 soup객체에  content 저장\n",
    "    #추출해야할 정보들 죄다 zip로 묶어줌 uid는 저위에 comp_uid\n",
    "\n",
    "    #회사 uid\n",
    "    #uid는 get_comp_info 함수에서 인자로 받음\n",
    "    \n",
    "    #회사이름\n",
    "    elements_comp_name = soup.find('div',class_=\"company_name\").find('a')\n",
    "    comp_name_temp=str(elements_comp_name).split('>')[1]\n",
    "    comp_name=comp_name_temp.split('<')[0] \n",
    "\n",
    "    #회사 주소\n",
    "    elements_comp_loc= soup.find('ul',class_='basic_info_more')#.find('dl',class_= 'info_item_more') #회사주소'\n",
    "    loc= elements_comp_loc.text.split('\\n'*5) #5개 개행문자로 구분자니까 5개의 개행문자를 제거해서 인덱스 순대로 정보 나오게 설정\n",
    "    comp_loc=loc[2].split('\\n')[1]  # 첫번째 줄을 제외한 부분을 선택\n",
    "\n",
    "    #회사 로고\n",
    "    elements_comp_thumb = soup.find('span',class_='img_wrap')\n",
    "    comp_thumb = str(elements_comp_thumb).split('\"')[5] # 큰 따옴표 기준으로 구분\n",
    "\n",
    "    #사업 내용\n",
    "    elements_comp_cont = soup.find('strong', class_ = 'info_item_subject')\n",
    "    #정규표현식으로 '>'와 '<' 사이에 있는 문자열 추출\n",
    "    pattern = re.search(r'>\\w+\\/\\w+\\/\\w+<', str(elements_comp_cont)).group()\n",
    "    comp_cont = pattern[1:-1]  # '<산업 내용>'\n",
    "    comp_cont = comp_cont.replace('<','').replace('>','')  # 산업내용만 추출\n",
    "\n",
    "    #회사 설립 년월\n",
    "    elements_comp_founded = soup.find('div', class_= 'basic_info_item ico_establish').find('strong')\n",
    "    pattern_2 = re.search(r'<strong[^>]*>(.*?)</strong>', str(elements_comp_founded))\n",
    "    comp_founded= pattern_2.group(1) #기업 규모만 추출\n",
    "\n",
    "    #회사 규모 (어떤 형태들이 있는지 확인 중소 대)\n",
    "    elements_comp_size = soup.find('div', class_= 'basic_info_item ico_company_shape').find('strong')\n",
    "    pattern_3 = re.search(r'<strong[^>]*>(.*?)</strong>', str(elements_comp_size))\n",
    "    comp_size= pattern_3.group(1) #기업 규모만 추출\n",
    "\n",
    "    #기업 대표 사이트\n",
    "    elements_comp_url = soup.find('ul', class_=\"basic_info_more\").find('a')\n",
    "    comp_url = str(elements_comp_url).split('\"')[1]\n",
    "    \n",
    "    comp_info_dict={\n",
    "        'comp_uid' : comp_uid,\n",
    "        'comp_name': comp_name,\n",
    "        'comp_loc' : comp_loc,\n",
    "        'comp_thumb' : comp_thumb,\n",
    "        'comp_cont' : comp_cont,\n",
    "        'comp_founded': comp_founded,\n",
    "        'comp_size' : comp_size,\n",
    "        'comp_url' : comp_url\n",
    "        \n",
    "    } #기업 정보 key, value로 담은 딕셔너리\n",
    "    return get_comp_info_df(comp_info_dict)#comp_name, comp_loc, comp_thumb, comp_cont, comp_founded, comp_size, comp_url #comp_thumb #comp_thumb #기업정보들 튜플로 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 크롤링 데이터 저장 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comp_info_df(comp_info_dict):\n",
    "    df=pd.DataFrame.from_dict({'':comp_info_dict}, orient='index') #데이터를 데이터프레임으로 저장\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------\n",
    "## main함수 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comp_uid</th>\n",
       "      <th>comp_name</th>\n",
       "      <th>comp_loc</th>\n",
       "      <th>comp_thumb</th>\n",
       "      <th>comp_cont</th>\n",
       "      <th>comp_founded</th>\n",
       "      <th>comp_size</th>\n",
       "      <th>comp_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>42217</td>\n",
       "      <td>네이버(주)</td>\n",
       "      <td>경기 성남시 분당구 불정로 6 (정자동,그린팩토리)</td>\n",
       "      <td>https://jpassets.jobplanet.co.kr/production/up...</td>\n",
       "      <td>포털/인터넷/콘텐츠</td>\n",
       "      <td>1999.06.02</td>\n",
       "      <td>중견기업</td>\n",
       "      <td>https://www.navercorp.com/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       " comp_uid comp_name                      comp_loc   \n",
       "    42217    네이버(주)  경기 성남시 분당구 불정로 6 (정자동,그린팩토리)  \\\n",
       "\n",
       "                                         comp_thumb   comp_cont comp_founded   \n",
       "  https://jpassets.jobplanet.co.kr/production/up...  포털/인터넷/콘텐츠   1999.06.02  \\\n",
       "\n",
       " comp_size                    comp_url  \n",
       "      중견기업  https://www.navercorp.com/  "
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_url('네이버') # 함수 잘 실행되는지 테스트\n",
    "#get_url(기업명)[0]은 기업정보 url 주소, get_url(기업명)[1]은 기업번호(uid로 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이제 url에 접속했으니 크롤링하는 함수 만들자 모듈화도 하기 나중에 get_url로 한번에 뽑을 수 있게 연결 반환값 넘겨줘서\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ezpz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

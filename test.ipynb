{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"csebuetnlp/mT5_m2m_crossSum_enhanced\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "MT5ForConditionalGeneration                                  --\n",
       "├─Embedding: 1-1                                             192,086,016\n",
       "├─MT5Stack: 1-2                                              192,086,016\n",
       "│    └─Embedding: 2-1                                        (recursive)\n",
       "│    └─ModuleList: 2-2                                       --\n",
       "│    │    └─MT5Block: 3-1                                    7,079,808\n",
       "│    │    └─MT5Block: 3-2                                    7,079,424\n",
       "│    │    └─MT5Block: 3-3                                    7,079,424\n",
       "│    │    └─MT5Block: 3-4                                    7,079,424\n",
       "│    │    └─MT5Block: 3-5                                    7,079,424\n",
       "│    │    └─MT5Block: 3-6                                    7,079,424\n",
       "│    │    └─MT5Block: 3-7                                    7,079,424\n",
       "│    │    └─MT5Block: 3-8                                    7,079,424\n",
       "│    │    └─MT5Block: 3-9                                    7,079,424\n",
       "│    │    └─MT5Block: 3-10                                   7,079,424\n",
       "│    │    └─MT5Block: 3-11                                   7,079,424\n",
       "│    │    └─MT5Block: 3-12                                   7,079,424\n",
       "│    └─MT5LayerNorm: 2-3                                     768\n",
       "│    └─Dropout: 2-4                                          --\n",
       "├─MT5Stack: 1-3                                              192,086,016\n",
       "│    └─Embedding: 2-5                                        (recursive)\n",
       "│    └─ModuleList: 2-6                                       --\n",
       "│    │    └─MT5Block: 3-13                                   9,439,872\n",
       "│    │    └─MT5Block: 3-14                                   9,439,488\n",
       "│    │    └─MT5Block: 3-15                                   9,439,488\n",
       "│    │    └─MT5Block: 3-16                                   9,439,488\n",
       "│    │    └─MT5Block: 3-17                                   9,439,488\n",
       "│    │    └─MT5Block: 3-18                                   9,439,488\n",
       "│    │    └─MT5Block: 3-19                                   9,439,488\n",
       "│    │    └─MT5Block: 3-20                                   9,439,488\n",
       "│    │    └─MT5Block: 3-21                                   9,439,488\n",
       "│    │    └─MT5Block: 3-22                                   9,439,488\n",
       "│    │    └─MT5Block: 3-23                                   9,439,488\n",
       "│    │    └─MT5Block: 3-24                                   9,439,488\n",
       "│    └─MT5LayerNorm: 2-7                                     768\n",
       "│    └─Dropout: 2-8                                          --\n",
       "├─Linear: 1-4                                                192,086,016\n",
       "=====================================================================================\n",
       "Total params: 966,573,312\n",
       "Trainable params: 966,573,312\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = r'''\n",
    "연차사용이 자유롭고 분위기가 느슨하다. 유연근무제 도입했다.\n",
    "연봉은 최악이며 미래먹거리가 없으며 저마다 처신하기 바쁨.\n",
    "제발 정신좀차리고 주인의식 좀 느끼세요. 그대들 배부르다고 직원들도 배부른줄아나?\n",
    "자유로운 연차 사용으로 원할때 사용하면 되며, 직원들을 위해서 다양한 소통 문화를 만들려고 함.\n",
    "개인 연차나 비활동시에도 시스템에서 바로 확인되었으면 좋겠고, 연차 시 확인이 안되서 업무 전화가 많이 옴.\n",
    "회사 직원들의 의견을 더 들어주셨으면 좋겠고, 적극적으로 반영해주셨으면 좋겠습니다.\n",
    "월급 안밀리고 밥나오고 연차를 눈치없이 쓸 수 있음.\n",
    "오래다니면 다닐수록 일이 더 쉬워짐.\n",
    "직원을 홀대하고 연봉을 올려주지 않는 반면 경력직들은 직전연봉 다 챙겨줘 가며 입사시킴.\n",
    "바라는 것도 없고 그냔 이대로만 해주세요. 잘하고 있다면서요?\n",
    "연차 자율제도와 유연근무 제도로 가정에 더 충실할 수 있고 삶이 생긴다.\n",
    "한량? 들이 많고 일을 하려는 의지들이 약하며 다들 불만만 많은 상황인데 인사정책은 더 역효과를 불러옴.\n",
    "본인들 평가나 성과를 위한 업무를 하지말고 밑에 사람들을 돌보아야 될듯.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = article_text.replace('\\n', ' ').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'device': 'cuda'} not recognized.\n"
     ]
    }
   ],
   "source": [
    "get_lang_id = lambda lang: tokenizer._convert_token_to_id(\n",
    "    model.config.task_specific_params[\"langid_map\"][lang][1]\n",
    ") \n",
    "\n",
    "target_lang = \"korean\" # for a list of available language names see below\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    [WHITESPACE_HANDLER(article_text)],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    ")[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\")\n",
    "input_ids = input_ids.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    decoder_start_token_id=get_lang_id(target_lang),\n",
    "    max_length=84,\n",
    "    no_repeat_ngram_size=1,\n",
    "    num_beams=4,\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tokenizer.decode(\n",
    "    output_ids,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<extra_id_70> 회사에서 연차 사용이 자유롭고 분위기가 느슨하다.\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sae4k_df(path='/home/parking/ml/data/MiniProj/data/sae4k/sae4k_v2.txt'):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sae4KDataset(Dataset):\n",
    "    def __init__(self, tokenizer, df):\n",
    "        self.__tokenizer = tokenizer\n",
    "        self.__df = df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모듈화 된 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.torchmodules.mt5sum import MT5Sum\n",
    "\n",
    "mt5sum = MT5Sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = r'''\n",
    "SK하이닉스 제공SK하이닉스는 SK스퀘어가 효율적인 해외 반도체 투자를 위해 설립한 투자법인 'TGC SQUARE'에 공동 출자했다고 4일 밝혔다. 투자법인은 1천억 원을 시작으로 추가 참여를 원하는 기업을 위해 공정출자 기회를 열어 두고 있다. SK하이닉스를 비롯해 신한금융그룹과 LIG넥스원 등이 공동 출자에 참여했다. SK하이닉스와 SK스퀘어는 독보적인 기술력을 가진 반도체 소부장 기업에 선제적으로 투자해 안정적인 글로벌 반도체 공급망을 구축하고, 첨단 기술 경쟁력을 강화할 계획이다. 글로벌 반도체 공급망은 경쟁적으로 자국 중심의 생태계 조성을 위해 급격하게 재편하고 있다. 이에 따라 글로벌 밸류체인 강화를 위해 △반도체 설계 △생산 △패키징 공정별 기술적 우위를 가진 소부장 기업화 협력이 필수라는 평가다. TGC SQUARE는 글로벌 탑티어 반도체 기업의 전문가가 기술 인사이트를 제공하는 '반도체 자문위원회'를 운영해 전문적인 투자심의 체계를 구축했다. 최우성 SK스퀘어 반도체 투자담당(MD) 겸 SK텔레콤 재팬 대표가 투자법인의 CEO(최고경영자)를 맡는다. 조희준 전 BNP파리바 일본법인 영업담당을 CIO(최고투자책임자)로, 미야모토 야스테루 전 크레디트스위스 부사장을 전문심사역으로 영입했다. SK하이닉스와 SK스퀘어는 첫 투자 대상으로 일본 반도체 강소기업을 검토하고 있다. 현재 조성된 투자금의 60%를 일본 소부장 기업에 투자한다는 방침이다. 일본은 전 세계 반도체 소부장 강자로 꼽힌다. △반도체 소재 △부품 △장비 등 소부장 영역에서 대체가 어려운 하이엔드 기술에 특화해 전 세계시장점유율 30%대를 차지하는 글로벌 1~2위 기업이 많다. SK하이닉스와 SK스퀘어는 일본 반도체 투자 네트워크를 가동해 △반도체 검사장비 개발 A사 △친환경 반도체 부품 제조 B사 △AI(인공지능) 반도체 개발 C사 △차세대 반도체 소재 개발 D사 등 잠재적 투자 대상 기업을 중심으로 기술검증을 진행할 예정이다. SK하이닉스와 SK스퀘어는 성장기업에 투자한 이후 향후 M&A(인수합병)와 IPO(기업공개)를 지원하는 방식도 검토하고 있다. TGC SQUARE 최우성 CEO는 \"글로벌 반도체 인사이트를 가진 SK 주요 관계사와 국내 대표 금융사 등이 해외 공동투자를 통해 국내외 반도체 산업의 생태계를 확장하는 유의미한 프로젝트\"라며 \"글로벌 유수의 소부장 기업과 협력을 확대해 미래 반도체 기술 기반을 다지는 계기가 될 것\"이라고 말했다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<extra_id_70> 글로벌 반도체 소부장 기업 SK하이닉스가 세계 최대 규모의 해외 공정출자 기회를 열었다.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt5sum.get_sum(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "news_df = pd.read_csv('/home/parking/ml/data/MiniProj/data/news/SK 하이닉스_naver.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parking/anaconda3/envs/ezpz/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.nocutnews.co.kr/news/5970546</td>\n",
       "      <td>SK하이닉스 제공SK하이닉스는 SK스퀘어가 효율적인 해외 반도체 투자를 위해 설립한...</td>\n",
       "      <td>글로벌 반도체 소부장 기업 SK하이닉스가 세계 최대 규모의 해외 공정출자 기회를 열었다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.etnews.com/20230704000192</td>\n",
       "      <td>SK가 일본 반도체 소재·부품·장비(소부장) 기술 확보에 나선다. SK하이닉스와 S...</td>\n",
       "      <td>글로벌 반도체 기업 SK하이닉스(SK)가 일본 소부장 기술 확보에 나선다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.hankyung.com/finance/article/20230...</td>\n",
       "      <td>사진=한경DB 이베스트투자증권은 5일 SK하이닉스 SK하이닉스 117,300 -0....</td>\n",
       "      <td>지난 5일 이베스트 투자증권은 SK하이닉스(S&amp;P)의 실적 추정치는 매출 7조300...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://news.mt.co.kr/mtview.php?no=20230704103...</td>\n",
       "      <td>SK하이닉스와 SK스퀘어가 국내 대표 금융사 등과 약 1000억원을 공동 출자해 일...</td>\n",
       "      <td>글로벌 반도체 기업 SK하이닉스가 일본과 미국 등 해외 유망기업에 약 1000억원을...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://n.news.naver.com/mnews/article/092/000...</td>\n",
       "      <td>투자법인 TGC SQUARE 설립...첫 투자 대상은 일본 소부장 기업SK하이닉스와...</td>\n",
       "      <td>일본의 반도체 소부장 기업 SK하이닉스(SK텔레콤 재팬)가 국내 대표 금융사 등과 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0           https://www.nocutnews.co.kr/news/5970546   \n",
       "1              https://www.etnews.com/20230704000192   \n",
       "2  https://www.hankyung.com/finance/article/20230...   \n",
       "3  http://news.mt.co.kr/mtview.php?no=20230704103...   \n",
       "4  https://n.news.naver.com/mnews/article/092/000...   \n",
       "\n",
       "                                             content  \\\n",
       "0  SK하이닉스 제공SK하이닉스는 SK스퀘어가 효율적인 해외 반도체 투자를 위해 설립한...   \n",
       "1  SK가 일본 반도체 소재·부품·장비(소부장) 기술 확보에 나선다. SK하이닉스와 S...   \n",
       "2  사진=한경DB 이베스트투자증권은 5일 SK하이닉스 SK하이닉스 117,300 -0....   \n",
       "3  SK하이닉스와 SK스퀘어가 국내 대표 금융사 등과 약 1000억원을 공동 출자해 일...   \n",
       "4  투자법인 TGC SQUARE 설립...첫 투자 대상은 일본 소부장 기업SK하이닉스와...   \n",
       "\n",
       "                                                 sum  \n",
       "0  글로벌 반도체 소부장 기업 SK하이닉스가 세계 최대 규모의 해외 공정출자 기회를 열었다.  \n",
       "1          글로벌 반도체 기업 SK하이닉스(SK)가 일본 소부장 기술 확보에 나선다.  \n",
       "2  지난 5일 이베스트 투자증권은 SK하이닉스(S&P)의 실적 추정치는 매출 7조300...  \n",
       "3  글로벌 반도체 기업 SK하이닉스가 일본과 미국 등 해외 유망기업에 약 1000억원을...  \n",
       "4  일본의 반도체 소부장 기업 SK하이닉스(SK텔레콤 재팬)가 국내 대표 금융사 등과 ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['sum'] = news_df['content'].apply(lambda x: mt5sum.get_sum(x))\n",
    "news_df['sum'] = news_df['sum'].apply(lambda x: x.replace('<extra_id_70>', '').strip())\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.to_csv('/home/parking/ml/data/MiniProj/data/news/SK 하이닉스_naver_sum.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bert모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-FinBert-SC\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"snunlp/KR-FinBert-SC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = 'SK하이닉스가 해외 유망 소부장 기업을 최소해 투자할 계획이다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tokenizer(input_str, return_tensors='pt')\n",
    "output = model(**input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_output = F.softmax(output.logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_output.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label[sm_output.argmax(dim=1).item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.torchmodules.FinBertKR import FinBertKR\n",
    "\n",
    "fbkr = FinBertKR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fbkr\u001b[39m.\u001b[39;49mpredict(input_str)\n",
      "File \u001b[0;32m~/ml/MiniProj/modules/torchmodules/FinBertKR.py:23\u001b[0m, in \u001b[0;36mFinBertKR.predict\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[1;32m     22\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__tokenizer(text, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m     24\u001b[0m     probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__softmax(outputs\u001b[39m.\u001b[39mlogits, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mid2label[probs\u001b[39m.\u001b[39margmax()\u001b[39m.\u001b[39mitem()]\n",
      "File \u001b[0;32m~/anaconda3/envs/ezpz/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/ezpz/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1555\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1556\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1562\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   1563\u001b[0m     input_ids,\n\u001b[1;32m   1564\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1565\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1566\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1567\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1568\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1569\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1570\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1571\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1572\u001b[0m )\n\u001b[1;32m   1574\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1576\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/ezpz/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/ezpz/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1013\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m   1014\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1015\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1016\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1017\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1018\u001b[0m     past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[1;32m   1019\u001b[0m )\n\u001b[1;32m   1020\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m   1021\u001b[0m     embedding_output,\n\u001b[1;32m   1022\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1031\u001b[0m )\n\u001b[1;32m   1032\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/ezpz/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/ezpz/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:230\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    227\u001b[0m         token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_ids\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embeddings(input_ids)\n\u001b[1;32m    231\u001b[0m token_type_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[1;32m    233\u001b[0m embeddings \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m token_type_embeddings\n",
      "File \u001b[0;32m~/anaconda3/envs/ezpz/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/ezpz/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/anaconda3/envs/ezpz/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "fbkr.predict(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.nocutnews.co.kr/news/5970546</td>\n",
       "      <td>SK하이닉스 제공SK하이닉스는 SK스퀘어가 효율적인 해외 반도체 투자를 위해 설립한...</td>\n",
       "      <td>글로벌 반도체 소부장 기업 SK하이닉스가 세계 최대 규모의 해외 공정출자 기회를 열었다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.etnews.com/20230704000192</td>\n",
       "      <td>SK가 일본 반도체 소재·부품·장비(소부장) 기술 확보에 나선다. SK하이닉스와 S...</td>\n",
       "      <td>글로벌 반도체 기업 SK하이닉스(SK)가 일본 소부장 기술 확보에 나선다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.hankyung.com/finance/article/20230...</td>\n",
       "      <td>사진=한경DB 이베스트투자증권은 5일 SK하이닉스 SK하이닉스 117,300 -0....</td>\n",
       "      <td>지난 5일 이베스트 투자증권은 SK하이닉스(S&amp;P)의 실적 추정치는 매출 7조300...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://news.mt.co.kr/mtview.php?no=20230704103...</td>\n",
       "      <td>SK하이닉스와 SK스퀘어가 국내 대표 금융사 등과 약 1000억원을 공동 출자해 일...</td>\n",
       "      <td>글로벌 반도체 기업 SK하이닉스가 일본과 미국 등 해외 유망기업에 약 1000억원을...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://n.news.naver.com/mnews/article/092/000...</td>\n",
       "      <td>투자법인 TGC SQUARE 설립...첫 투자 대상은 일본 소부장 기업SK하이닉스와...</td>\n",
       "      <td>일본의 반도체 소부장 기업 SK하이닉스(SK텔레콤 재팬)가 국내 대표 금융사 등과 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0           https://www.nocutnews.co.kr/news/5970546   \n",
       "1              https://www.etnews.com/20230704000192   \n",
       "2  https://www.hankyung.com/finance/article/20230...   \n",
       "3  http://news.mt.co.kr/mtview.php?no=20230704103...   \n",
       "4  https://n.news.naver.com/mnews/article/092/000...   \n",
       "\n",
       "                                             content  \\\n",
       "0  SK하이닉스 제공SK하이닉스는 SK스퀘어가 효율적인 해외 반도체 투자를 위해 설립한...   \n",
       "1  SK가 일본 반도체 소재·부품·장비(소부장) 기술 확보에 나선다. SK하이닉스와 S...   \n",
       "2  사진=한경DB 이베스트투자증권은 5일 SK하이닉스 SK하이닉스 117,300 -0....   \n",
       "3  SK하이닉스와 SK스퀘어가 국내 대표 금융사 등과 약 1000억원을 공동 출자해 일...   \n",
       "4  투자법인 TGC SQUARE 설립...첫 투자 대상은 일본 소부장 기업SK하이닉스와...   \n",
       "\n",
       "                                                 sum  \n",
       "0  글로벌 반도체 소부장 기업 SK하이닉스가 세계 최대 규모의 해외 공정출자 기회를 열었다.  \n",
       "1          글로벌 반도체 기업 SK하이닉스(SK)가 일본 소부장 기술 확보에 나선다.  \n",
       "2  지난 5일 이베스트 투자증권은 SK하이닉스(S&P)의 실적 추정치는 매출 7조300...  \n",
       "3  글로벌 반도체 기업 SK하이닉스가 일본과 미국 등 해외 유망기업에 약 1000억원을...  \n",
       "4  일본의 반도체 소부장 기업 SK하이닉스(SK텔레콤 재팬)가 국내 대표 금융사 등과 ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/parking/ml/data/MiniProj/data/news/SK 하이닉스_naver_sum.csv', encoding='utf-8-sig')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>sum</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.nocutnews.co.kr/news/5970546</td>\n",
       "      <td>SK하이닉스 제공SK하이닉스는 SK스퀘어가 효율적인 해외 반도체 투자를 위해 설립한...</td>\n",
       "      <td>글로벌 반도체 소부장 기업 SK하이닉스가 세계 최대 규모의 해외 공정출자 기회를 열었다.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.etnews.com/20230704000192</td>\n",
       "      <td>SK가 일본 반도체 소재·부품·장비(소부장) 기술 확보에 나선다. SK하이닉스와 S...</td>\n",
       "      <td>글로벌 반도체 기업 SK하이닉스(SK)가 일본 소부장 기술 확보에 나선다.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.hankyung.com/finance/article/20230...</td>\n",
       "      <td>사진=한경DB 이베스트투자증권은 5일 SK하이닉스 SK하이닉스 117,300 -0....</td>\n",
       "      <td>지난 5일 이베스트 투자증권은 SK하이닉스(S&amp;P)의 실적 추정치는 매출 7조300...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://news.mt.co.kr/mtview.php?no=20230704103...</td>\n",
       "      <td>SK하이닉스와 SK스퀘어가 국내 대표 금융사 등과 약 1000억원을 공동 출자해 일...</td>\n",
       "      <td>글로벌 반도체 기업 SK하이닉스가 일본과 미국 등 해외 유망기업에 약 1000억원을...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://n.news.naver.com/mnews/article/092/000...</td>\n",
       "      <td>투자법인 TGC SQUARE 설립...첫 투자 대상은 일본 소부장 기업SK하이닉스와...</td>\n",
       "      <td>일본의 반도체 소부장 기업 SK하이닉스(SK텔레콤 재팬)가 국내 대표 금융사 등과 ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0           https://www.nocutnews.co.kr/news/5970546   \n",
       "1              https://www.etnews.com/20230704000192   \n",
       "2  https://www.hankyung.com/finance/article/20230...   \n",
       "3  http://news.mt.co.kr/mtview.php?no=20230704103...   \n",
       "4  https://n.news.naver.com/mnews/article/092/000...   \n",
       "\n",
       "                                             content  \\\n",
       "0  SK하이닉스 제공SK하이닉스는 SK스퀘어가 효율적인 해외 반도체 투자를 위해 설립한...   \n",
       "1  SK가 일본 반도체 소재·부품·장비(소부장) 기술 확보에 나선다. SK하이닉스와 S...   \n",
       "2  사진=한경DB 이베스트투자증권은 5일 SK하이닉스 SK하이닉스 117,300 -0....   \n",
       "3  SK하이닉스와 SK스퀘어가 국내 대표 금융사 등과 약 1000억원을 공동 출자해 일...   \n",
       "4  투자법인 TGC SQUARE 설립...첫 투자 대상은 일본 소부장 기업SK하이닉스와...   \n",
       "\n",
       "                                                 sum sentiment  \n",
       "0  글로벌 반도체 소부장 기업 SK하이닉스가 세계 최대 규모의 해외 공정출자 기회를 열었다.  negative  \n",
       "1          글로벌 반도체 기업 SK하이닉스(SK)가 일본 소부장 기술 확보에 나선다.  negative  \n",
       "2  지난 5일 이베스트 투자증권은 SK하이닉스(S&P)의 실적 추정치는 매출 7조300...  negative  \n",
       "3  글로벌 반도체 기업 SK하이닉스가 일본과 미국 등 해외 유망기업에 약 1000억원을...  negative  \n",
       "4  일본의 반도체 소부장 기업 SK하이닉스(SK텔레콤 재팬)가 국내 대표 금융사 등과 ...  negative  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'] = df['sum'].apply(lambda x: classifier(x)[0]['label'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/home/parking/ml/data/MiniProj/data/news/SK 하이닉스_naver_sum_sentiment.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ezpz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

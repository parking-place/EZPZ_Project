{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#새로운 회사 크롤링 모듈 \n",
    "\"\"\"\n",
    "새로운 회사 리스트를 받아서 \n",
    "1.기업정보 크롤링 요청 및 테이블 생성\n",
    "2.뉴스정보 크롤링 요청및 테이블 생성\n",
    "3.채용 공고도 크롤링\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "sys.path.append('/app/EZPZ_Project/modules/crawlers/news') # 뉴스 정보 크롤러 경로\n",
    "sys.path.append('/app/EZPZ_Project/modules/torchmodules') # 토치 모델 뉴스 요약 및 감정평가 가져오기\n",
    "sys.path.append('/app/EZPZ_Project/modules/crawlers/job_post') #채용공고 크롤러\n",
    "sys.path.append('/app/EZPZ_Project/modules/crawlers/comp_info') #기업 정보 크롤러\n",
    "sys.path.append('/app/EZPZ_Project') #db 연동정보 경로\n",
    "\n",
    "import pymysql\n",
    "import socket\n",
    "import cryptography\n",
    "\n",
    "from service_models import ServiceModels\n",
    "from finbertkr import FinBertKR\n",
    "from mt5sum import MT5Sum\n",
    "from t5basesum import T5BaseSum\n",
    "\n",
    "\n",
    "\n",
    "import daum_news_crawler #다음뉴스크롤러\n",
    "import naver_news_crawler #네이버 뉴스크롤러\n",
    "import news_crawlers #네이버 뉴스 크롤러2\n",
    "import wanted_recruit_crawler #채용공고 크롤러\n",
    "import info_crawler #기업정보 크롤러\n",
    "from privates.ezpz_db import * #db연동 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "if conn.db ==b'testdb':\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'testdb'\n"
     ]
    }
   ],
   "source": [
    "print(str(conn.db)) #이런식으로 호출이야 dict 값은 알았지?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'ezpzlmsz'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = get_connection('test')\n",
    "cur = conn.cursor()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_check= ServiceModels() #모델 서빙 모듈 객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('desc comp_news') #key연결 순서대로 다 drop 해버리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute('select * from comp_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute('select * from comp_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute('select * from recruit_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_list= ['삼성전자(주)','(주)카카오','네이버(주)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute('delete from comp_info') #언제든지 삭제가능 위치 뒤바꿔야 될수도 있음\n",
    "cur.execute('delete from comp_news')\n",
    "cur.execute('delete from recruit_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "cur.execute('select * from recruit_info order by comp_uid')\n",
    "for i in cur:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실행시작\n",
      "삼성전자\n",
      "56\n",
      "카카오\n",
      "57\n",
      "네이버\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "crawler_exec(comp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#여기서 함수를 실행해서 각종 정보들 실행\n",
    "def crawler_exec(comp_list):\n",
    "    comp_info_crawl_save(comp_list)\n",
    "    #comp_news_crawl_save(comp_list)\n",
    "    recruit_info_crawl(comp_list)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#먼저 comp_info 크롤링해와서 테이블에 넣어줘야겠지? 근데 no니까 is_reged 전부 y로 바꿔주고 나중에 테이블 전부다\n",
    "def comp_info_crawl_save(comp_list):\n",
    "    for comp in comp_list:\n",
    "        comp_info_df=info_crawler.get_url(comp) #info_crawler 기업정보 데이터 프레임 잘 어울림\n",
    "        # 기업 정보 리스트로 저장해서 데이터프레임에 넣어줄수 있게\n",
    "        col_value=[]\n",
    "        for i in comp_info_df.iloc[0]:\n",
    "            col_value.append(i) #테이블에 insert할 수 있는 컬럼값들을 리스트화\n",
    "\n",
    "\n",
    "        col_value[5]=col_value[5][0:7]\n",
    "        col_value[5] = col_value[5].replace(\".\", \"\")\n",
    "        col_value[5] # 6글자 문자열로 변환 테이블에 형식대로\n",
    "\n",
    "        #크롤링해온 값 테이블에 저장 저장일자와 수정일자는 스케줄링단계에서 진행이므로 일단 000000 넣어두었음\n",
    "        #일단 testdb를 경로로했는데 나중에 바꿔줘야됨\n",
    "        cur.execute(f\"INSERT INTO testdb.comp_info (comp_name, comp_loc, comp_thumb, comp_cont, comp_founded, comp_size, comp_url, is_reged, create_date, modify_date) VALUES (%s, %s, %s, %s, %s, %s, %s, 'N', '000000', '000000')\", (col_value[1], col_value[2], col_value[3], col_value[4], col_value[5], col_value[6], col_value[7]))      \n",
    "        \n",
    "        #값들을 전부 넣어줬으니 update y로\n",
    "        cur.execute('UPDATE comp_info SET is_reged = \"Y\" ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#얘 잘되는지는 .py 파일에서 확인\n",
    "def comp_news_crawl_save(comp_list):\n",
    "        print('실행시작')\n",
    "        for comp in comp_list:\n",
    "            daum_news = daum_news_crawler.get_news(comp).head(1) #다음뉴스 크롤러 실행 확인\n",
    "            naver_news = naver_news_crawler.get_news(comp).head(1) #네이버 뉴스크롤러 실행\n",
    "            all_news = pd.concat([daum_news, naver_news], ignore_index=True)  #뉴스 전체 합치기\n",
    "            print(all_news)\n",
    "            for index, col in enumerate(all_news['news_cont']):\n",
    "                if len(col)>5000:\n",
    "                    all_news['news_cont'].iloc[index] = col[:5000] #5000자 이상은 cut이므로 이걸로 체크\n",
    "\n",
    "            #모델 돌리기\n",
    "            cont_sum_list= [] #df에 넣어줄 요약 리스트\n",
    "            cont_sent_list=[] # df에 넣어줄 감정평가 리스트\n",
    "            senti_to_int=[] # 감정을 정수형으로 바꿔줄 리스트\n",
    "\n",
    "            for text in all_news['news_cont']:\n",
    "                cont_sum=data_check.get_summary(text, 'news')\n",
    "                cont_sum_list.append(cont_sum)\n",
    "                \n",
    "            \n",
    "            for text in cont_sum_list:\n",
    "                cont_sent=data_check.get_sentiment(text)\n",
    "                cont_sent_list.append(cont_sent)\n",
    "                # df_news_senti에 값을 0(중립), 1(긍정), 2(부정)으로 바꿔줘야함\n",
    "            \n",
    "            for col in (cont_sent_list):\n",
    "                if col =='neutral':\n",
    "                    senti_to_int.append(0)\n",
    "                elif col =='positive':\n",
    "                    senti_to_int.append(1)\n",
    "                else:\n",
    "                    senti_to_int.append(2)\n",
    "            \n",
    "            #데이터프레임에 요약 결과와 감정평가 결과 넣어주기\n",
    "            all_news['news_sum'] = cont_sum_list\n",
    "            all_news['news_senti'] = senti_to_int\n",
    "\n",
    "            for index,col in enumerate(all_news['news_sum']):\n",
    "                if len(col)>256:\n",
    "                    all_news['news_sum'].iloc[index] = col[:256] #5000자 이상은 cut이므로 이걸로 체크\n",
    "\n",
    "            #기사 내용과 요약본에 따옴표들 전부 삭제 전처리\n",
    "            clean_cont=[]\n",
    "            clean_sum=[]\n",
    "            for i in all_news['news_cont']:\n",
    "                \n",
    "                cont_clean = i.replace('\"', '').replace(\"'\", '')\n",
    "                clean_cont.append(cont_clean)\n",
    "                \n",
    "\n",
    "\n",
    "            for j in all_news['news_sum']:\n",
    "                sum_clean = j.replace('\"', '').replace(\"'\", '')\n",
    "                clean_sum.append(sum_clean)\n",
    "\n",
    "            all_news['news_cont']= clean_cont\n",
    "            all_news['news_sum'] = clean_sum         \n",
    "            \n",
    "            get_comp_news_db(all_news,comp) #실행될때마다 바뀌는 기업별 all_news 테이블화 시키기\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comp_news_db(all_news,comp): # 만들어진 데이터프레임을 테이블로\n",
    "    cur.execute(f'select comp_uid from comp_info where comp_name = \"{comp}\"') \n",
    "    comp_uid=cur.fetchall()[0][0]\n",
    "    # news_uid는 auto increment니까 자동생성되지 않을까?\n",
    "    for index, row in all_news.iterrows():\n",
    "            sql = 'insert into comp_news '\n",
    "            sql += '    (comp_uid, pub_date, news_url, news_cont,news_sum, news_senti, create_date, modify_date) '\n",
    "            sql += 'values ( '\n",
    "            sql += f'   \"{comp_uid}\", \"{row[\"pub_date\"]}\", \"{row[\"news_url\"]}\", \"{row[\"news_cont\"]}\", \"{row[\"news_sum\"]}\", \"{row[\"news_senti\"]}\" '\n",
    "            sql += f'    , \"{\"00000000\"}\", \"{\"00000000\"}\" '\n",
    "            sql += ') '\n",
    "            cur.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recruit_info_crawl(comp_list):\n",
    "    print('실행시작')\n",
    "    for comp in comp_list:\n",
    "        #채용공고는 주 붙어있으면 안됨 제거 전처리\n",
    "        recruit_comp = comp.replace('(주)',\"\")\n",
    "        print(recruit_comp)\n",
    "        recruit_info_df=wanted_recruit_crawler.get_recruit_info(recruit_comp, csv_save=False) # 원티드 기업정보 크롤러 모듈\n",
    "        new_i=[] #집합인 uid를 int로 바꿔준 값을 넣어준 리스트/ 테이블에 넣기위한 전처리\n",
    "        for i in range(len(recruit_info_df['recruit_uid'])):\n",
    "            new_i.append(list(recruit_info_df['recruit_uid'][i])[0])\n",
    "        recruit_info_df['recruit_uid']=new_i #int 값으로 컬럼 대체\n",
    "\n",
    "\n",
    "        cur.execute(f'select comp_uid from comp_info where comp_name = \"{comp}\"') \n",
    "        comp_uid=cur.fetchall()[0][0]\n",
    "        print(comp_uid)\n",
    "        for index, row in recruit_info_df.iterrows():\n",
    "            sql = 'insert into recruit_info '\n",
    "            sql += '    (comp_uid, recruit_uid, recruit_url, recruit_position, recruit_thumb, create_date, modify_date) '\n",
    "            sql += 'values ( '\n",
    "            sql += f'   \"{comp_uid}\", \"{row[\"recruit_uid\"]}\", \"{row[\"recruit_url\"]}\", \"{row[\"recruit_position\"]}\", \"{row[\"recruit_thumb\"]}\" '\n",
    "            sql += f'    , \"{\"00000000\"}\", \"{\"00000000\"}\" '\n",
    "            sql += ') '\n",
    "            cur.execute(sql)\n",
    "            for i in cur:\n",
    "                print(i)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뉴스 감성분석 model POC\n",
    "\n",
    "- 모델 출처:[snunlp/KR-FinBert · Hugging Face](https://huggingface.co/snunlp/KR-FinBert)\n",
    "- train 데이터셋 출처:\n",
    "    1. [finance_sentiment_corpus/finance_data.csv at main · ukairia777/finance_sentiment_corpus          (github.com)](https://github.com/ukairia777/finance_sentiment_corpus/blob/main/finance_data.csv) \n",
    "    2. [통계청_인공지능 학습을 위한 고용기사 감성지수 라벨링 데이터_20201229 | 공공데이터포털 (data.go.kr)](https://www.data.go.kr/data/15075840/fileData.do)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 사용 모델: krfinbert-model -hugging face\n",
    "\n",
    "### 파인튜닝을 위한 train data set:\n",
    "\n",
    "1. finance sentiment dataset - github\n",
    "2. 고용기사 감성 지수 라벨링 dataset - 공공데이터포털"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파인튜닝 개요\n",
    "\n",
    "먼저 dataset을 불러오고 가져온 dataset을 학습에 사용할 수 있게 3가지 sentiment(긍정,부정,중립)들을 각각 1(긍정), 0(중립), -1(부정)으로 라벨링해주었다. 또 finetuning에 쓰일 Dataset을 모델이 사용할 수 있는 데이터로 전처리하는  함수(ReviewDataset 함수)를 만들고 가져온 dataset을 dataloader를 통해  batch 단위로 모델에 사용할 수 있게 가져온 뒤 사전학습 모델을 받아와 Finetuning 해주는 NET 클래스를 구현하였다. 그리고 구현한 NET 클래스를 통해 실제로 Load한 dataset을 학습시켜줬다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finetuning 코드 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 필요한 모듈 불러오기\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 불러오기\n",
    "df = pd.read_csv(DICT_PATH+r\"/df_news_sentiment\",encoding= 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.데이터 전처리\n",
    "\n",
    "sent_list=[] #긍정 부정 중립을 각 2 0 -1로 바꿔서 담아줌\n",
    "for sent in df['sentiment']:\n",
    "    if sent =='positive':\n",
    "        sent_list.append(2)\n",
    "\n",
    "    elif sent =='neutral':\n",
    "        sent_list.append(1)\n",
    "\n",
    "    else:\n",
    "        sent_list.append(0)        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>기사</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gran에 따르면, 그 회사는 회사가 성장하고 있는 곳이지만, 모든 생산을 러시아로...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>테크노폴리스는 컴퓨터 기술과 통신 분야에서 일하는 회사들을 유치하기 위해 10만 평...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>국제 전자산업 회사인 엘코텍은 탈린 공장에서 수십 명의 직원을 해고했으며, 이전의 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>새로운 생산공장으로 인해 회사는 예상되는 수요 증가를 충족시킬 수 있는 능력을 증가...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-2012년 회사의 업데이트된 전략에 따르면, Basware는 20% - 4...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12459</th>\n",
       "      <td>코로나 충격에 따른 근로 시간 감소_ 과거 위기의 5배</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12460</th>\n",
       "      <td>7월 사업체종사자 13만8천명 감소 코로나19 여파에 5개월째 감소</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12461</th>\n",
       "      <td>한치 앞도 안보인다..채용문 꽁꽁 닫는 카드사</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12462</th>\n",
       "      <td>토스_ 고객 상담직 신입 및 경력 30명 공개 채용</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12463</th>\n",
       "      <td>토스_ 연봉 3000만원 고객 상담직 공개채용</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12464 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      기사  sentiment\n",
       "0      Gran에 따르면, 그 회사는 회사가 성장하고 있는 곳이지만, 모든 생산을 러시아로...          1\n",
       "1      테크노폴리스는 컴퓨터 기술과 통신 분야에서 일하는 회사들을 유치하기 위해 10만 평...          1\n",
       "2      국제 전자산업 회사인 엘코텍은 탈린 공장에서 수십 명의 직원을 해고했으며, 이전의 ...          0\n",
       "3      새로운 생산공장으로 인해 회사는 예상되는 수요 증가를 충족시킬 수 있는 능력을 증가...          2\n",
       "4      2009-2012년 회사의 업데이트된 전략에 따르면, Basware는 20% - 4...          2\n",
       "...                                                  ...        ...\n",
       "12459                     코로나 충격에 따른 근로 시간 감소_ 과거 위기의 5배          0\n",
       "12460              7월 사업체종사자 13만8천명 감소 코로나19 여파에 5개월째 감소          0\n",
       "12461                          한치 앞도 안보인다..채용문 꽁꽁 닫는 카드사          0\n",
       "12462                       토스_ 고객 상담직 신입 및 경력 30명 공개 채용          2\n",
       "12463                          토스_ 연봉 3000만원 고객 상담직 공개채용          2\n",
       "\n",
       "[12464 rows x 2 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "2    4870\n",
       "0    4007\n",
       "1    3587\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터의 각sentiment 별 비율 분포 확인 \n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POC를 위해 train과 target데이터에서 10개 정도만 분리해서 구현이 가능한지만 확인\n",
    "train=df['기사'][:10].to_numpy()\n",
    "target = df[\"sentiment\"][:10].to_numpy(dtype=np.float32).reshape(-1,1) #(-1,1)을 써서 [0,1,0....,1] 이런식인 target 배열을 [[0][1][0]...[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.Dataset을 불러오고 모델이 사용할수 있는 형태로 전처리해주는 함수 구현\n",
    "\n",
    "class ReviewDataset(torch.utils.data.Dataset): #이 데이터셋을 반복적으로 사용해 데이터프레임의 리뷰들을 하나하나 체크\n",
    "    def __init__(self ,tokenizer , x, y = None ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.x = x #입력값\n",
    "        self.y = y #정답\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        item[\"x\"] = self.__tokenizer(self.x[idx]) #리뷰 각각을 토크나이저 해주기 위한 코드\n",
    "        if self.y is not None: #정답이 있다면\n",
    "            item[\"y\"] = torch.Tensor(self.y[idx])#그 리뷰의 정답을 텐서로 변환\n",
    "        return item #토큰화된 리뷰와 그에 맞는 텐서 데이터인 정답을 함께 반환\n",
    "    def __tokenizer(self,text):\n",
    "        inputs = self.tokenizer(text, add_special_tokens=True,padding='max_length', truncation=True,max_length = 37)\n",
    "        for k, v in inputs.items():\n",
    "            #k는 input_ids와 token_type_ids 그리고 어텐션마스크이며 v는 그 k(키)의 각 항목에 따른 v(벨류)를 저장\n",
    "            #v는 각 값을 저장한 리스트\n",
    "            inputs[k] = torch.LongTensor(v) #longtensor로 바꿔준 value값들을 inputs[k]로 저장\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.사전학습 모델 선언\n",
    "# Load model directly krfinbert 모델 파인튜닝 위해서 갖고옴\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "model_name = \"snunlp/KR-FinBert-SC\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001C8E2FD6680>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x': {'input_ids': tensor([[    2,    43,  5328, 10178,  5051, 10053,    16,  2078, 17805, 14243,\n",
       "          10384,  8455,  8494, 14275,  8482,    16,  8703,  9716,  5008,  9873,\n",
       "           5016,  3625,  5131,  9714,  5022, 10019,  8544,    18,     3,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])},\n",
       " 'y': tensor([[1.]])}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5.각 토큰화된 뉴스들을 dt 변수에 담아 dl 변수에 담아줌\n",
    "dt = ReviewDataset(tokenizer,train,target) #리뷰데이터셋 객체 각 행(리뷰)들을 토큰화하고 정답 매칭해서 저장\n",
    "dl = torch.utils.data.DataLoader(dt, batch_size=1,shuffle=False) #데이터 로더: 데이터셋 객체를 받아 미니배치형태로 구현하는 제네레이터\n",
    "batch = next(iter(dl))\n",
    "batch #이게 리뷰하나를 얘기하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': {'input_ids': tensor([[    2,    43,  5328, 10178,  5051, 10053,    16,  2078, 17805, 14243,\n",
       "          10384,  8455,  8494, 14275,  8482,    16,  8703,  9716,  5008,  9873,\n",
       "           5016,  3625,  5131,  9714,  5022, 10019,  8544,    18,     3,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])},\n",
       " 'y': tensor([[1.]])}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(dl))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"x\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits'])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**batch[\"x\"]) #batch x 의 3가지 key타입들을 dict로 model에 넣어줌\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.파인튜닝(NET)/ 모델 구현\n",
    "class Net(torch.nn.Module):\n",
    "    #def __init__(self, model_name):\n",
    "    #    super().__init__()\n",
    "    #    self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    #    self.output_layer = torch.nn.Linear(self.model.config.hidden_size, 1)\n",
    "\n",
    "    def __init__(self,model_name): #밑에서 module 받아야 하니\n",
    "        super().__init__()\n",
    "        model_name = \"snunlp/KR-FinBert-SC\"\n",
    "        self.__tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.__model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.__model.to(DEVICE)\n",
    "        #self.__softmax = F.softmax 일단은 주석 처리 손실함수때문\n",
    "\n",
    "    def forward(self, text): #여기서 text를 받아 토큰화 시키고 output을 받으며 probs로 비선형화후 제일높은 확률의 친구를 pred에 넣어서 반환\n",
    "        #x = self.model(**x)\n",
    "        #x = self.output_layer(x[1])\n",
    "        #text는 이미 token화 돼있음\n",
    "        #inputs = self.__tokenizer(text, return_tensors='pt').to(DEVICE) 이미 토큰화돼있으므로\n",
    "        #outputs = self.__model(**inputs)\n",
    "        outputs= self.__model(**text) #토큰화된 text의 key들을 모델에 넘겨줌\n",
    "        probs=outputs.logits\n",
    "        ##print(probs)\n",
    "        ##probs = self.__softmax(outputs.logits, dim=1) #output logit의 3개의 값을 활성화함수에 넣어줌\n",
    "        ##print(probs.argmax().item())\n",
    "        ##print(self.__model.config.id2label)\n",
    "        #pred = self.__model.config.id2label[probs.argmax().item()].keys() #id2label 은 답이 담겨있는 dict 0:negative 1:neutral 2:positive\n",
    "        \n",
    "        #pred=pred.to(torch.long)\n",
    "        #pred=torch.tensor(pred) 요거 써야됨\n",
    "        #pred=torch.tensor(pred).reshape(1,1)\n",
    "        #따라서 배치사이즈 1로 해서 한번 할 때마다 3개 중에 하나 고르게 하기 또한 pred에는 positive(str)이 저장되므로 value로 바꿔줌\n",
    "        #잠깐 어차피 확률 3개중 제일 높은게 item이라면 계속 넣어주면되는거 아님\n",
    "        return probs\n",
    "        #return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (_Net__model): BertForSequenceClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(20000, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net(model_name) #버트 베이스드 모델을 이용해 추가 학습 시키는것\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.target data를 원핫인코딩 해주는 함수 구현\n",
    "def ohe_batch(batch): #batch['y']를 원핫인코딩해주는 함수 [-1.]은  [0,0,1] [0.]-> [0,1,0] [1.] -> [0,0,1]\n",
    "    # tensor 0. =>\n",
    "    if batch == torch.tensor([[0.]]):\n",
    "        batch =torch.tensor([[1,0,0]])\n",
    "        return batch \n",
    "        \n",
    "    elif batch == torch.tensor([[1.]]):\n",
    "        batch = torch.tensor([[0,1,0]])\n",
    "        return batch \n",
    "    else:\n",
    "        batch = torch.tensor([[0,0,1]]) \n",
    "        return batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.train/test 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader,model,loss_fn,optimizer,device):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(dataloader):\n",
    "        pred = model(batch[\"x\"].to(device))\n",
    "        \n",
    "        target_batch_ohe =ohe_batch(batch['y'])\n",
    "        #loss = loss_fn(pred, batch[\"y\"].to(device))\n",
    "        \n",
    "        target_batch_ohe = torch.argmax(target_batch_ohe, dim=1) # crossentropy를 쓸때는 원핫인코딩 벡터대신 정수형(long) 인덱스를 넘겨주어야함\n",
    "        target_batch_ohe = target_batch_ohe.to(torch.long)\n",
    "        #원핫인코딩된 인덱스의 번호(위치)가 저장\n",
    "        \n",
    "        loss = loss_fn(pred, target_batch_ohe.to(device))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(dataloader)\n",
    "\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_loop(dataloader,model,loss_fn,device):\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "\n",
    "    pred_list = []\n",
    "    sig = torch.nn.Sigmoid()\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "\n",
    "        pred = model(batch[\"x\"].to(device))\n",
    "        if batch.get(\"y\") is not None:\n",
    "            target_batch_ohe =ohe_batch(batch['y'])\n",
    "            #loss = loss_fn(pred, batch[\"y\"].to(device))\n",
    "            #print(target_batch_ohe,1) \n",
    "            #print(pred,2)\n",
    "            target_batch_ohe = torch.argmax(target_batch_ohe, dim=1) # crossentropy를 쓸때는 원핫인코딩 벡터대신 정수형(long) 인덱스를 넘겨주어야함\n",
    "            target_batch_ohe = target_batch_ohe.to(torch.long)\n",
    "            loss = loss_fn(pred, target_batch_ohe.to(device))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        pred = sig(pred) #시그모이드 함수에 넣어 선형->비선형\n",
    "        pred = pred.to(\"cpu\").numpy()\n",
    "        \n",
    "        pred_list.append(pred)\n",
    "\n",
    "    epoch_loss /= len(dataloader)\n",
    "\n",
    "    pred = np.concatenate(pred_list)\n",
    "    print('pred:',pred)\n",
    "    \n",
    "    return epoch_loss , pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.학습을 위한 하이퍼파라미터 정의\n",
    "n_splits = 5\n",
    "epochs = 4 #20\n",
    "batch_size = 1\n",
    "loss_fn = torch.nn.CrossEntropyLoss() #다중 분류 하려면 크로스엔트로피LOSS를 사용해야함 SOFTMAX가 안에서 이루어지므로 하면 안됨\n",
    "SEED=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cv = KFold(n_splits=n_splits,shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KFold 방식으로 train data와 validation dataset을 나누어주고 실제로 학습을 진행\n",
    "is_holdout = True\n",
    "#reset_seeds(SEED)\n",
    "best_score_list = []\n",
    "for i,(tri,vai) in enumerate(cv.split(train)): #kfold로 나누어준 곳에서 train과 validation set 순서대로 뽑아줌\n",
    "\n",
    "    model = Net(model_name).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    train_dt = ReviewDataset(tokenizer,train[tri],target[tri])\n",
    "    valid_dt = ReviewDataset(tokenizer,train[vai],target[vai])\n",
    "    train_dl = torch.utils.data.DataLoader(train_dt, batch_size=batch_size, shuffle=True)\n",
    "    valid_dl = torch.utils.data.DataLoader(valid_dt, batch_size=batch_size,shuffle=False)\n",
    "    best_score = 0\n",
    "    patience = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_loss = train_loop(train_dl, model, loss_fn,optimizer,DEVICE )\n",
    "    \n",
    "        valid_loss , pred = test_loop(valid_dl, model, loss_fn,DEVICE  ) #test_loop에서 개같이 반환해주고 있으\n",
    "        pred = (pred > 0.5).astype(int)\n",
    "        print('벨리데이션 정답값:',target[vai],'예측값:',pred) #pred가 왜 1 0 1로 나올까\n",
    "        score = accuracy_score(target[vai],pred )\n",
    "        \n",
    "        patience += 1\n",
    "        print(train_loss,valid_loss,score,sep=\"\\t\")\n",
    "        if best_score < score:\n",
    "            patience = 0\n",
    "            best_score = score\n",
    "            torch.save(model.state_dict(),f\"model_{i}.pth\")\n",
    "\n",
    "        if patience == 3:\n",
    "            break\n",
    "\n",
    "    best_score_list.append(best_score)\n",
    "\n",
    "    if is_holdout:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ezpz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
